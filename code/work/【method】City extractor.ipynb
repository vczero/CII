{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c58e61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "from collections import Counter\n",
    "import jieba\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d9c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('./data/20221007_20231006_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef11da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/sa340.csv')\n",
    "with open ('./data/family_name.txt') as f:\n",
    "    family_name = f.read()\n",
    "family_names = family_name.split(',')\n",
    "family_names = family_names[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78fe4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stopwords(city_name):\n",
    "    data = []\n",
    "    # common sense\n",
    "    data.append(city_name + '路')\n",
    "    data.append(city_name + '东路')\n",
    "    data.append(city_name + '西路')\n",
    "    data.append(city_name + '南路')\n",
    "    data.append(city_name + '北路')\n",
    "    data.append(city_name + '中路')\n",
    "    data.append(city_name + '街')\n",
    "    data.append(city_name + '镇')\n",
    "    data.append(city_name + '县')\n",
    "    data.append(city_name + '区')\n",
    "    data.append(city_name + '村')\n",
    "\n",
    "    \n",
    "    data.append(city_name + '公园')\n",
    "    data.append(city_name + '先生')\n",
    "    data.append(city_name + '大道')\n",
    "    data.append(city_name + '一路')\n",
    "    data.append(city_name + '二路')\n",
    "    data.append(city_name + '三路')\n",
    "    data.append(city_name + '四路')\n",
    "    data.append(city_name + '五路')\n",
    "    data.append(city_name + '六路')\n",
    "    data.append(city_name + '七路')\n",
    "    data.append(city_name + '八路')\n",
    "    data.append(city_name + '九路')\n",
    "    data.append(city_name + '十路')\n",
    "    data.append(city_name + '环路')\n",
    "    data.append(city_name + '的路')\n",
    "    data.append(city_name + '东一路')\n",
    "    data.append(city_name + '西一路')\n",
    "    data.append(city_name + '南一路')\n",
    "    data.append(city_name + '北一路')\n",
    "    data.append(city_name + '东二路')\n",
    "    data.append(city_name + '西二路')\n",
    "    data.append(city_name + '南二路')\n",
    "    data.append(city_name + '北二路')\n",
    "    data.append(city_name + '东街')\n",
    "    data.append(city_name + '西街')\n",
    "    data.append(city_name + '南街')\n",
    "    data.append(city_name + '北街')\n",
    "    data.append(city_name + '支路')\n",
    "    data.append(city_name + '桥')\n",
    "    data.append(city_name + '外国语学校')\n",
    "    \n",
    "    # 过滤底部无关新闻的超链接\n",
    "    data.append('@' + city_name)\n",
    "    data.append(city_name + '广场')\n",
    "    data.append(city_name + '池')\n",
    "    \n",
    "    # special common sense\n",
    "    data.append('吉林省')\n",
    "    data.append('北京时间')\n",
    "    data.append('入海口')\n",
    "    data.append('海口岸')\n",
    "    data.append('延安精神')\n",
    "    data.append('重大理论')\n",
    "    data.append('亚运城')\n",
    "    data.append('日照时长')\n",
    "    data.append('东西部')\n",
    "    \n",
    "    data.append('南海东') \n",
    "    data.append('海东西') \n",
    "    data.append('海东南')\n",
    "    data.append('海东北')\n",
    "    data.append('海南部')\n",
    "    \n",
    "    data.append('南海西')\n",
    "    data.append('海西部')\n",
    "    data.append('海西北')\n",
    "    data.append('海西南')\n",
    "    data.append('海西区')\n",
    "    data.append('海西湖')\n",
    "    \n",
    "    data.append('海北部')\n",
    "    data.append('渤海北')\n",
    "    data.append('南海北')\n",
    "    data.append('东海北')\n",
    "    data.append('海口岸')\n",
    "    data.append('出海口')\n",
    "    \n",
    "     # extra knowledge\n",
    "    data.append('三明治')\n",
    "    data.append('端午安康')\n",
    "    data.append('孙中山')\n",
    "    data.append('适中')\n",
    "    data.append('重庆小面')\n",
    "    data.append('重庆麻辣')\n",
    "    data.append('重庆面庄')\n",
    "    data.append('隆重庆祝')\n",
    "    data.append('长春百子图')\n",
    "    data.append('春分')\n",
    "    data.append('兰州拉面')\n",
    "    data.append('兰州牛肉面')\n",
    "    data.append('兰州牛肉拉面')\n",
    "    data.append('兰州传统牛肉面')\n",
    "    data.append('澳大利亚昆士兰州')\n",
    "    data.append('马里兰州')\n",
    "    data.append('宁德时代')\n",
    "    data.append('山地') \n",
    "    \n",
    "    # add hundred family names\n",
    "    for f in family_names:\n",
    "        data.append(f + city_name)\n",
    "        \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af6201a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in df['city_shortname']:\n",
    "    name = i.split('+')[0]\n",
    "    data += build_stopwords(name)\n",
    "    \n",
    "data = list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc225cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49380"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16427a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data, columns=['word']).to_csv('./data/city_stopwords2.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e5cb86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mapping = pd.read_csv('./data/sa340.csv')\n",
    "pro_names = list(set(df_mapping['city_shortname']))\n",
    "len(pro_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cb21d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(pd.read_csv('./data/city_stopwords2.csv')['word'])\n",
    "\n",
    "def replace_citys_stop_words(text, stopwords):\n",
    "    if 'xxx' in text:\n",
    "        return ''\n",
    "    \n",
    "    # 因为编辑的外链习惯是不同的\n",
    "    \n",
    "    if '来源' in text:\n",
    "        text = text.split('来源')[0]\n",
    "        \n",
    "    if '编辑' in text:\n",
    "        text = text.split('编辑')[0]\n",
    "        \n",
    "    if '往期阅读' in text:\n",
    "        text = text.split('往期阅读')[0]\n",
    "        \n",
    "        \n",
    "    if '推荐阅读' in text:\n",
    "        text = text.split('推荐阅读')[0]\n",
    "    \n",
    "        \n",
    "    for i in stopwords:\n",
    "        text = text.replace(i, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_city_loc_name_arr(text, names, exclude_city='0'):\n",
    "    words = []\n",
    "    text = str(text)\n",
    "    \n",
    "    \n",
    "    for name in names:\n",
    "        name_arr = name.split('+')\n",
    "        # 针对多个地名简称\n",
    "        # exclude_city 可能是存在多个简称的情况 +，因此必须是 not in \n",
    "        results = []\n",
    "        for j in name_arr:\n",
    "            if j in text:\n",
    "                # 1.替换\n",
    "                text = replace_citys_stop_words(text, stopwords)\n",
    "                # 2.分词\n",
    "                cut_ts = jieba.lcut(text)\n",
    "                \n",
    "                if j in cut_ts and j not in exclude_city:\n",
    "                    results.append(j)\n",
    "                     \n",
    "        if len(results):\n",
    "            words.append(name)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    if len(words):\n",
    "        return list(set(words))\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "\"\"\"\n",
    "city abbreviation to city full name\n",
    "\"\"\"\n",
    "def shortname2name(shortname):\n",
    "    return list(set(df_mapping[df_mapping['city_shortname'] == shortname]['name']))[0]\n",
    "\n",
    "\"\"\"\n",
    "city full name to city abbreviation\n",
    "\"\"\"\n",
    "def name2shortname(name):\n",
    "    return list(set(df_mapping[df_mapping['name'] == name]['city_shortname']))[0]\n",
    "\n",
    "\"\"\"\n",
    "sort the aggregated cities\n",
    "\"\"\"\n",
    "def Counter_citylink(citylinks):\n",
    "    c = Counter(citylinks)\n",
    "    cc = sorted(c.items(), key=lambda x:x[1] , reverse=True)\n",
    "    return cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "014dbcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of times a city reports on other cities\n",
    "def get_city_links(query_name):\n",
    "    city_csv = df_all[df_all.city_name==query_name]\n",
    "    shortname = name2shortname(query_name)\n",
    "    gzh_citys = []\n",
    "    for i, r in city_csv.iterrows():\n",
    "        citys = get_city_loc_name_arr(r['text'], pro_names, shortname)\n",
    "        gzh_citys += citys\n",
    "        \n",
    "    v = Counter_citylink(gzh_citys)\n",
    "    return [(shortname2name(i[0]), i[1]) for i in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d4e849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "source_city: \n",
    "target_citys: \n",
    "\"\"\"\n",
    "def get_aja_list (source_city, target_citys):\n",
    "    links = get_city_links(source_city)\n",
    "    links = dict(links)\n",
    "    data = []\n",
    "    for n in target_citys:\n",
    "        count = links.get(n) or 0\n",
    "        data.append(count)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebb8c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必须按照citys做邻接矩阵，否则顺序乱了\n",
    "# citys = list(set(df_mapping['name']))\n",
    "# citys.sort(reverse=True)\n",
    "# data = []\n",
    "# for i in tqdm(citys):\n",
    "#     data.append(get_aja_list(i, citys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c804811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(data, columns=citys)\n",
    "# df['city'] = citys\n",
    "# df = df.set_index('city')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "382b6051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('./study_data/cities_gzh_aja_martix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3e391fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49380"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ea44d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
